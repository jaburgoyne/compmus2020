---
title: "Week 11 Â· Novelty Functions and Tempograms"
author: "John Ashley Burgoyne"
date: "11 March 2020"
output: 
    html_notebook:
        # theme: flatly
        css: 'css/bootstrap-theme.min.css'
---

## Set-up

```{r, results = 'hide'}
library(tidyverse)
library(spotifyr)
library(compmus)
source('spotify.R')
```

## Novelty Functions

For novelty functions, we want to work directly with the segments, and not summarise them at higher levels like Spotify's own estimates of bar or beat.

```{r}
pata_pata <- 
    get_tidy_audio_analysis('3uy90vHHATPjtdilshDQDt') %>% 
    select(segments) %>% unnest(segments)
```

We can compute an energy-based novelty function based on Spotify's loudness estimates. The tempo of this piece is about 126 BPM: how well does this technique work?

```{r}
pata_pata %>% 
    mutate(loudness_max_time = start + loudness_max_time) %>% 
    arrange(loudness_max_time) %>% 
    mutate(delta_loudness = loudness_max - lag(loudness_max)) %>% 
    ggplot(aes(x = loudness_max_time, y = pmax(0, delta_loudness))) +
    geom_line() +
    xlim(0, 30) +
    theme_minimal() +
    labs(x = 'Time (s)', y = 'Novelty')
```

We can use similar approaches for chromagrams and cepstrograms. In the case of chromagrams, Aitchison's clr transformation gives more sensible differences between time points. Even with these helpful transformations, however, self-similarity matrices tend to be more helpful visualisations of chroma and timbre from the Spotify API.

```{r}
pata_pata %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    arrange(start) %>% 
    mutate(pitches = map2(pitches, lag(pitches), `-`)) %>% 
    compmus_gather_chroma %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = pitch_class, 
            fill = pmax(0, value))) + 
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    xlim(0, 30) +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    theme_classic()
```


```{r}
pata_pata %>% 
    arrange(start) %>% 
    mutate(timbre = map2(timbre, lag(timbre), `-`)) %>% 
    compmus_gather_timbre %>% 
    ggplot(
        aes(
            x = start + duration / 2, 
            width = duration, 
            y = basis, 
            fill = pmax(0, value))) + 
    geom_tile() +
    scale_fill_viridis_c(option = 'E', guide = 'none') +
    xlim(0, 30) +
    labs(x = 'Time (s)', y = NULL, fill = 'Magnitude') +
    theme_classic()
```


Find a Spotify track that has a regular tempo but lacks percussion (e.g., much Western classical music), and compute the above three representations. How do they differ from what you see for 'Pata Pata'?

## Tempograms

Spotify does not make the novelty function underlying their own tempo analysis available to the public, but we can still use onsets of every segment to generate Fourier tempograms. The `tempogram()` function from `compmus` generates this automatically from an audio analysis, ready to plot with `geom_raster` (a faster version of `geom_tile` for when every segment has the same length). Here is an example of 'Samba do outro lugar', a track from the Brazilian indie band Graveola that features several tempo and metre alternations.

```{r}
graveola <- get_tidy_audio_analysis('6PJasPKAzNLSOzxeAH33j2')
```

```{r}
graveola %>% 
    tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) %>% 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
    labs(x = 'Time (s)', y = 'Tempo (BPM)') +
    theme_classic()
```

The textbook notes that Fourier-based tempograms tend to pick up strongly on tempo harmonics. Wrapping into a cyclic tempogram can be more informative. 

```{r}
graveola %>% 
    tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>% 
    ggplot(aes(x = time, y = bpm, fill = power)) + 
    geom_raster() + 
    scale_fill_viridis_c(guide = 'none') +
    labs(x = 'Time (s)', y = 'Tempo (BPM)') +
    theme_classic()
```

## Deltas and Delta-Deltas for Playlists (Advanced)

Let's try to identify some of the features that Spotify uses to designate playlists as 'workout' playlists. For a full analysis, we would need to delve deeper, but let's start with a comparison of three playlists: Indie Pop, Indie Party, and Indie Workout. For speed, this example will work with only the first 20 songs from each playlist, but you should feel free to use more if your computer can handle it.

```{r}
pop <- 
    get_playlist_audio_features('spotify', '37i9dQZF1DWWEcRhUVtL8n') %>% 
    slice(1:20) %>% 
    add_audio_analysis
party <- 
    get_playlist_audio_features('spotify', '37i9dQZF1DWTujiC7wfofZ') %>% 
    slice(1:20) %>% 
    add_audio_analysis
workout <- 
    get_playlist_audio_features('spotify', '37i9dQZF1DXaRL7xbcDl7X') %>% 
    slice(1:20) %>% 
    add_audio_analysis
```

As you think about the rest of this lab session -- and your portfolio -- think about the four kinds of validity that Sturm and Wiggins discussed in our reading for this week. Do these projects have:

  - Statistical validity [somewhat beyond the scope of this course]?
  - Content validity?
  - Internal validity?
  - External validity?

We bind the three playlists together using the trick from Week 7, transpose the chroma vectors to a common tonic using the `compmus_c_transpose` function, and then summarise the vectors like we did when generating chromagrams and cepstrograms. Again, Aitchison's clr transformation can help with chroma.

```{r}
indie <- 
    pop %>% mutate(playlist = "Indie Pop") %>% 
    bind_rows(
        party %>% mutate(playlist = "Indie Party"),
        workout %>% mutate(playlist = "Indie Workout")) %>% 
    mutate(playlist = factor(playlist)) %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        pitches = 
            map(segments, 
                compmus_summarise, pitches, 
                method = 'mean', norm = 'manhattan'),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean')) %>% 
    mutate(pitches = map(pitches, compmus_normalise, 'clr')) %>% 
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(cols = c(pitches, timbre))
```

Although the novelty-based transformations of chroma and timbre features are not always useful for visualisations, they can be very useful for classification (next week). Both 'deltas' and 'delta-deltas', especially for timbre features, are in regular use in music information retrieval. The code example below shows how to compute average *delta* chroma and timbre features instead of the ordinary average. Can you incorporate it into the classifiers above? Can you add delta-deltas, too? Can you use a visualisation to find any patterns in the data?

```{r}
indie_deltas <-
    pop %>% mutate(playlist = "Indie Pop") %>% 
    bind_rows(
        party %>% mutate(playlist = "Indie Party"),
        workout %>% mutate(playlist = "Indie Workout")) %>% 
    mutate(playlist = factor(playlist)) %>% 
    mutate(
        segments = 
            map2(segments, key, compmus_c_transpose)) %>% 
    mutate(
        segments = 
            map(
                segments, 
                mutate, 
                pitches = map(pitches, compmus_normalise, 'manhattan'))) %>% 
    mutate(
        segments =
            map(
                segments,
                mutate,
                pitches = map2(pitches, lag(pitches), `-`))) %>% 
    mutate(
        segments =
            map(
                segments,
                mutate,
                timbre = map2(timbre, lag(timbre), `-`))) %>% 
    mutate(
        pitches =
            map(segments,
                compmus_summarise, pitches,
                method = 'mean', na.rm = TRUE),
        timbre =
            map(
                segments,
                compmus_summarise, timbre,
                method = 'mean', na.rm = TRUE)) %>%
    mutate_at(vars(pitches, timbre), map, bind_rows) %>% 
    unnest(cols = c(pitches, timbre))
```
